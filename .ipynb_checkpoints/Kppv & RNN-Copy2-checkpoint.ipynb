{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a071d8d5",
   "metadata": {},
   "source": [
    "# TD1 – Kppv et réseaux de neurones pour la classification d'images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b47e65",
   "metadata": {},
   "source": [
    "L’objectif de ce TD est d'implémenter un programme Python complet de classification d'images. Deux modèles de classification seront abordés : les k-plus-proches-voisins (kppv) et les réseaux de neurones (RNN). Les module numpy et scikit image seront utilisés, respectivement pour la manipulation des matrices et la manipulation des images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e6a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kppv import *\n",
    "from skimage.feature import hog\n",
    "from skimage import data, exposure\n",
    "rng = np.random.default_rng()\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.feature import hog\n",
    "from skimage.feature import local_binary_pattern\n",
    "np.random.seed(1) # pour que l'exécution soit déterministe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b369478",
   "metadata": {},
   "source": [
    "# Réseaux de neurones"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3abd53",
   "metadata": {},
   "source": [
    "On s'intéresse désormais aux modèles de classification basé sur un réseau de neurones. On réutilise les fonctions de lecture et de découpage des données réalisées précédemment. Par ailleurs, commençons par définir les fonctions classiques calculatoires utilisées par la suite. Nous définissons la fonction sigmoid bien que nous n'utiliserons que les fonctions ReLU et softmax, respectivement pour les couches cachées et la couche de sortie. On définit également leur fonction backward correspondantes, qui nous permettront de réaliser la descente du gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3dd2395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    cache = Z\n",
    "    Z -= np.max(Z)\n",
    "    sm = (np.exp(Z) / np.sum(np.exp(Z), axis=0))\n",
    "    return sm, cache\n",
    "\n",
    "def sigmoid(Z):\n",
    "    A = 1 / (1 + np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    A = np.maximum(0, Z)\n",
    "    assert (A.shape == Z.shape)\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def softmax_backward(dA, cache):\n",
    "    z = cache\n",
    "    z -= np.max(z)\n",
    "    s = (np.exp(z) / np.sum(np.exp(z), axis=0))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, cache):\n",
    "    Z = cache\n",
    "    s = 1 / (1 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)  # just converting dz to a correct object.\n",
    "    # When z <= 0, we set dz to 0 as well.\n",
    "    dZ[Z <= 0] = 0\n",
    "    assert (dZ.shape == Z.shape)\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc9a48",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "On commence par définir une fonction, qui, à partir d'une liste contenant les dimensions des différentes couches du réseau de neurones, initialisera les paramètres.\n",
    "On peut noter ici que l'on réalise une initialisation des poids du réseau (matrice W) selon les principes Kaiming / MSRA Initialization. En effet, ce type d'initialisation est plus performante dans le cadre de l'utilisation de fonction ReLU pour les couches cachées du réseau. On initialise les biais (matrice b) comme étant un vecteur nul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cf5ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2/layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1ae34",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "Ensuite on définit 3 fonctions nécessaires à la propagation en avant. Une fonction **linear_forward** effectuant les produits matriciels de la propagation en avant, puis une fonction **linear_activation_forward**, qui, utilisant **linear_forward** effectue la propagation d'une couche à une autre (produit matriciel + fonction d'activation). On a 3 options de fonctions : relu, sigmoid ou softmax. Par la suite nous utiliserons la fonctions **ReLU** pour les couches cachées, et la fonction **softmax** pour générer les probabilités sur la couche de sortie.\n",
    "Enfin, **L_model_forward** permet de réaliser la propagation à travers toutes les couches, en utilisant **linear_activation_forward**. C'est le seul endroit où un boucle *for* est inévitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5908b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_normalization(X):\n",
    "    eps = 1e-7\n",
    "    var = np.var(X, axis=0, keepdims=True)\n",
    "    mean = np.mean(X, axis=0, keepdims=True)\n",
    "    X_n = (X - mean)/np.sqrt(var + eps)\n",
    "    return X_n\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    A = batch_normalization(A)\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "    for l in range(1, L + 1): # L+1 is pretty important ATTENTION\n",
    "        A_prev = A\n",
    "        Wl = parameters['W' + str(l)]\n",
    "        bl = parameters['b' + str(l)]\n",
    "        if l < L:\n",
    "            A, cache = linear_activation_forward(A_prev, Wl, bl, activation=\"relu\")\n",
    "        else:\n",
    "            AL, cache = linear_activation_forward(A_prev, Wl, bl, activation=\"softmax\")\n",
    "        caches.append(cache)\n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0559fc5",
   "metadata": {},
   "source": [
    "## Fonction de coût\n",
    "On définit une fonction de coût. On a J = $\\frac{-1}{m}$ $\\sum_{i=1}^{m} y_i*log(al_i) $ où AL est le vecteur de sortie obtenue lors de la propagation en avant. C'est la fonction de coût dite de 'cross-entropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23e8bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, activation_out, parameters, lambda_reg):\n",
    "    m = Y.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "    if activation_out == \"sigmoid\":\n",
    "        cost = (-1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "    elif activation_out == \"softmax\":\n",
    "        cost = (-1 / m) * np.sum(Y * np.log(AL))\n",
    "    reg_cost = 0\n",
    "    for l in range(1, L):\n",
    "        reg_cost += np.sum(parameters['W' + str(l)]**2)\n",
    "    return cost+ lambda_reg*reg_cost/m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efcd97f",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "On définit 3 fonctions nécessaires à la backpropagation. Une fonction **linear_backward** effectuant les produits matriciels de la backpropagation, puis une fonction **linear_activation_backward**, qui, utilisant **linear_backward** propage la backpropagation en fonctione de la fonction d'activation de la couche (utilisation de la fonction backward adéquate).\n",
    "Les différentes fonctions backward sont définies dans le fichier function.py, afin de ne pas alourdir le propos ici. Elles prennent toutes dA en argument, ainsi que le cache, et retournent dZ.\n",
    "Enfin, **L_model_backward** permet de réaliser la backpropagation à travers toutes les couches, en utilisant **linear_activation_backward**. C'est le seul endroit où un boucle *for* est inévitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3512cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    grads['dA' + str(L-1)], grads['dW' + str(L)], grads['db' + str(L)] = linear_activation_backward(dAL, caches[L-1], activation=\"softmax\")  # Softmax or sigmoid, in function of the need\n",
    "    for l in reversed(range(L-1)):\n",
    "        grads['dA' + str(l)], grads['dW' + str(l+1)], grads['db' + str(l+1)] = linear_activation_backward(grads['dA' + str(l+1)], caches[l], activation=\"relu\")\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677a039",
   "metadata": {},
   "source": [
    "## Mise à jour des paramètres\n",
    "Suite à la backpropagation, on définit une fonction pour mettre à jour les paramètres en fonctions des gradients récupérés, des valeurs initiales des paramètres, ainsi que du learning_rate. Ce learning_rate est un hyper-paramètre qu'il nous faudra ajuster par la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9277ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_grad(grad, beta1, beta2, n_iter): #n_iter is the number of the iteration/ epoch ?\n",
    "    first_moment = 0\n",
    "    second_moment = 0\n",
    "    first_moment = beta1 * first_moment+(1-beta1)*grad\n",
    "    second_moment = beta2* second_moment + (1-beta2)*grad*grad\n",
    "    first_unbias = first_moment / (1-beta1**n_iter)\n",
    "    second_unbias = second_moment / (1-beta2**n_iter)\n",
    "    return (first_unbias, second_unbias)\n",
    "\n",
    "def learning_rate_decay(learning_rate_0, n_epoch, N_epoch):\n",
    "    learning_rate_n = learning_rate_0*(1+np.cos(np.pi*n_epoch/N_epoch))/2  # Commence à 1\n",
    "    return learning_rate_n\n",
    "\n",
    "\n",
    "def update_parameters(params, grads, learning_rate, n_epoch, N_epoch, beta1, beta2, learning_r_decay=True, adam=True):\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    if learning_r_decay:\n",
    "        learning_rate_cur = learning_rate_decay(learning_rate, n_epoch, N_epoch)\n",
    "    else:\n",
    "        learning_rate_cur = learning_rate\n",
    "    for l in range(1,L+1):\n",
    "        if adam:\n",
    "            first_unbias_w, second_unbias_w = adam_grad(grads['dW' + str(l)], beta1, beta2, n_epoch)\n",
    "            parameters['W' + str(l)] = parameters['W' + str(l)] -learning_rate*first_unbias_w / (np.sqrt(second_unbias_w)+ 1e-7)\n",
    "            first_unbias_l, second_unbias_l = adam_grad(grads['db' + str(l)], beta1, beta2, n_epoch)\n",
    "            parameters['b' + str(l)] = parameters['b' + str(l)] -learning_rate*first_unbias_l / (np.sqrt(second_unbias_l)+ 1e-7)\n",
    "        else:\n",
    "            parameters['W' + str(l)] = parameters['W' + str(l)] -learning_rate_cur*grads['dW' + str(l)]\n",
    "            parameters['b' + str(l)] = parameters['b' + str(l)] -learning_rate_cur*grads['db' + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9ec0a",
   "metadata": {},
   "source": [
    "## Modèle complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "abddcf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_layer_model\n",
    "def L_layer_model(X, Y, parameters, learning_rate, lambda_reg, n_epoch, N_epoch, beta1, beta2, prev_cost,\n",
    "                  learning_r_dec, adam):\n",
    "\n",
    "    costn_1 = prev_cost\n",
    "    AL, caches = L_model_forward(X, parameters)\n",
    "    cost = compute_cost(AL, Y, 'softmax', parameters, lambda_reg)\n",
    "    grads = L_model_backward(AL, Y, caches)\n",
    "    parameters = update_parameters(parameters, grads, learning_rate, n_epoch, N_epoch, beta1, beta2, learning_r_dec, adam)\n",
    "\n",
    "    if costn_1 < cost:\n",
    "        print(\"Error, Alpha parameter to lessen\")\n",
    "\n",
    "    return parameters, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0570539",
   "metadata": {},
   "source": [
    "## Fonction d'évaluation de la performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6775674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(Ypred, Ytest):\n",
    "    tot_true = np.sum(Ytest==np.array(np.argmax(Ypred, axis=0)))\n",
    "    return tot_true/Ytest.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd256a8",
   "metadata": {},
   "source": [
    "## Tracé de l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fb4643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_function(cost):\n",
    "    n_iter = len(cost)\n",
    "    iter = np.arange(1, n_iter+1)\n",
    "    plt.plot(iter, cost)\n",
    "    plt.title(\"Cost function\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracy(train_accu, test_accu):\n",
    "    n_iter = len(train_accu)\n",
    "    iter = np.arange(1, n_iter+1)\n",
    "    plt.plot(iter, train_accu, \"b--\", label=\"Training accuracy\")\n",
    "    plt.plot(iter, test_accu, \"r--\", label=\"Test accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbcad9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "np.random.seed(1)  # pour que l'exécution soit déterministe\n",
    "\n",
    "data, labels = read_cifar(short=False)\n",
    "data = (data - np.mean(data, axis=0, keepdims=True))/np.std(data, axis=0, keepdims=True)\n",
    "Xapp, Yapp, Xtest, Ytest = split_data(data, labels)\n",
    "\n",
    "X_train = Xapp.T\n",
    "Y_train = np.array([Yapp])\n",
    "X_test_clean = Xtest.T\n",
    "Y_test_clean = np.array([Ytest])\n",
    "\n",
    "def NN_model(X_train, Y_train, X_test_clean, Y_test_clean, N_epoch, m_batch, learning_rate, lambda_reg, \n",
    "              beta1=0.9, beta2 = 0.999, learning_r_dec= True, adam=True):\n",
    "    n_x, m = X_train.shape[0], X_train.shape[1]\n",
    "    assert m % m_batch ==0\n",
    "    n_batch = m // m_batch\n",
    "    L_X_train = np.split(X_train, n_batch, axis=1)\n",
    "    L_Y_train = np.split(Y_train, n_batch, axis=1)\n",
    "    uniques = np.unique(Yapp)\n",
    "    n_y = len(uniques)\n",
    "    costs = []\n",
    "    train_accu = []\n",
    "    test_accu = []\n",
    "    layers_dims = [n_x, 50, 30, n_y] # 0.0025 good for [50,25]  # Try 0.0001 bcs 0.0005 not working at 1500 it.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    for k in range(1,N_epoch+1):\n",
    "        for j in range(n_batch):\n",
    "            Y_train_final = np.zeros((n_y, L_Y_train[j].shape[1]))\n",
    "            cost = np.inf\n",
    "            for i in range(n_y):\n",
    "                Y_train_final[i,:] = (L_Y_train[j] == uniques[i])\n",
    "                \n",
    "            parameters, cost = L_layer_model(L_X_train[j], Y_train_final, parameters, learning_rate, lambda_reg, \n",
    "                                   k, N_epoch, beta1, beta2, cost, learning_r_dec, adam)\n",
    "            costs.append(cost)\n",
    "            \n",
    "            pred_train = evaluate_prediction(L_model_forward(L_X_train[j],parameters)[0], Y_train_final)\n",
    "            pred_test = evaluate_prediction(L_model_forward(X_test_clean,parameters)[0], Y_test_clean)\n",
    "            train_accu.append(pred_train)\n",
    "            test_accu.append(pred_test)\n",
    "            print(cost)\n",
    "        print(f'NN accuracy on training set \\033[1m{pred_train:.3%}\\033[0m')\n",
    "        print(f'NN accuracy on test set \\033[1m{pred_test:.3%}\\033[0m')\n",
    "        plot_loss_function(cost)\n",
    "        plot_accuracy(train_accu, test_accu)\n",
    "\n",
    "#NN_model(X_train, Y_train, X_test_clean, Y_test_clean, N_epoch= 100, m_batch = 128, learning_rate=0.000000001, lambda_reg=0,\n",
    "#         beta1=0.9, beta2 = 0.999, learning_r_dec= False, adam=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48c6d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)  # pour que l'exécution soit déterministe\n",
    "\n",
    "data, labels = read_cifar(short=False)\n",
    "data = (data - np.mean(data, axis=0, keepdims=True))/np.std(data, axis=0, keepdims=True)\n",
    "Xapp, Yapp, Xtest, Ytest = split_data(data, labels)\n",
    "\n",
    "X_train = Xapp.T\n",
    "Y_train = np.array([Yapp])\n",
    "X_test_clean = Xtest.T\n",
    "Y_test_clean = np.array([Ytest])\n",
    "n_x, m = X_train.shape[0], X_train.shape[1]\n",
    "assert m % m_batch ==0\n",
    "n_batch = m // m_batch\n",
    "L_X_train = np.split(X_train, n_batch, axis=1)\n",
    "L_Y_train = np.split(Y_train, n_batch, axis=1)\n",
    "uniques = np.unique(Yapp)\n",
    "n_y = len(uniques)\n",
    "costs = []\n",
    "train_accu = []\n",
    "test_accu = []\n",
    "layers_dims = [n_x, 50, 30, n_y] # 0.0025 good for [50,25]  # Try 0.0001 bcs 0.0005 not working at 1500 it.\n",
    "parameters = initialize_parameters_deep(layers_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8e9700cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = read_cifar(short=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2dfaee4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'costs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_30192/1684451425.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcosts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'costs' is not defined"
     ]
    }
   ],
   "source": [
    "costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eae293",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.std(data, axis=0, keepdims=True).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c58c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8899a946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74deaadc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0257979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd7c4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df704011",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
