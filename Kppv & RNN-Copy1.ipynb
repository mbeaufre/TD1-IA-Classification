{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a071d8d5",
   "metadata": {},
   "source": [
    "# TD1 – Kppv et réseaux de neurones pour la classification d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e6a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kppv import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc9a48",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "On commence par définir une fonction, qui, à partir d'une liste contenant les dimensions des différentes couches du réseau de neurones, initialisera les paramètres.\n",
    "Pour avoir une initialisation reproductible on fixe np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf5ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2/layer_dims[l-1])\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1ae34",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "Ensuite on définit 3 fonctions nécessaires à la propagation en avant. Une fonction **linear_forward** effectuant les produits matriciels de la propagation en avant, puis une fonction **linear_activation_forward**, qui, utilisant **linear_forward** effectue la propagation d'une couche à une autre (produit matriciel + fonction d'activation). On a 3 options de fonctions : relu, sigmoid ou softmax. Par la suite nous utiliserons la fonctions **ReLU** pour les couches cachées, et la fonction **softmax** pour générer les probabilités sur la couche de sortie.\n",
    "Enfin, **L_model_forward** permet de réaliser la propagation à travers toutes les couches, en utilisant **linear_activation_forward**. C'est le seul endroit où un boucle *for* est inévitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5908b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    std = np.std(A, axis=0, keepdims=True)\n",
    "    mean = np.mean(A, axis=0, keepdims=True)\n",
    "    A = (A - mean)/std\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b,std)\n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "    for l in range(1, L + 1): # L+1 is pretty important ATTENTION\n",
    "        A_prev = A\n",
    "        Wl = parameters['W' + str(l)]\n",
    "        bl = parameters['b' + str(l)]\n",
    "        if l < L:\n",
    "            A, cache = linear_activation_forward(A_prev, Wl, bl, activation=\"relu\")\n",
    "        else:\n",
    "            # AL, cache = linear_activation_forward(A_prev, Wl, bl, activation=\"sigmoid\")\n",
    "            AL, cache = linear_activation_forward(A_prev, Wl, bl, activation=\"softmax\")\n",
    "        caches.append(cache)\n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0559fc5",
   "metadata": {},
   "source": [
    "## Fonction de coût\n",
    "On définit une fonction de coût. On a J = $\\frac{-1}{m}$ $\\sum_{i=1}^{m} y_i*log(al_i) $ où AL est le vecteur de sortie obtenue lors de la propagation en avant. C'est la fonction de coût dite de 'cross-entropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e8bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, activation_out, parameters, lambda_reg):\n",
    "    m = Y.shape[1]\n",
    "    L = len(parameters) // 2\n",
    "    if activation_out == \"sigmoid\":\n",
    "        cost = (-1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "    elif activation_out == \"softmax\":\n",
    "        cost = (-1 / m) * np.sum(Y * np.log(AL))\n",
    "    reg_cost = 0\n",
    "    for l in range(1, L):\n",
    "        reg_cost += np.sum(parameters['W' + str(l)]**2)\n",
    "    return cost+ lambda_reg*reg_cost/m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efcd97f",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "On définit 3 fonctions nécessaires à la backpropagation. Une fonction **linear_backward** effectuant les produits matriciels de la backpropagation, puis une fonction **linear_activation_backward**, qui, utilisant **linear_backward** propage la backpropagation en fonctione de la fonction d'activation de la couche (utilisation de la fonction backward adéquate).\n",
    "Les différentes fonctions backward sont définies dans le fichier function.py, afin de ne pas alourdir le propos ici. Elles prennent toutes dA en argument, ainsi que le cache, et retournent dZ.\n",
    "Enfin, **L_model_backward** permet de réaliser la backpropagation à travers toutes les couches, en utilisant **linear_activation_backward**. C'est le seul endroit où un boucle *for* est inévitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3512cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b, std = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)*std\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    grads['dA' + str(L-1)], grads['dW' + str(L)], grads['db' + str(L)] = linear_activation_backward(dAL, caches[L-1], activation=\"softmax\")  # Softmax or sigmoid, in function of the need\n",
    "    for l in reversed(range(L-1)):\n",
    "        grads['dA' + str(l)], grads['dW' + str(l+1)], grads['db' + str(l+1)] = linear_activation_backward(grads['dA' + str(l+1)], caches[l], activation=\"relu\")\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677a039",
   "metadata": {},
   "source": [
    "## Mise à jour des paramètres\n",
    "Suite à la backpropagation, on définit une fonction pour mettre à jour les paramètres en fonctions des gradients récupérés, des valeurs initiales des paramètres, ainsi que du learning_rate. Ce learning_rate est un hyper-paramètre qu'il nous faudra ajuster par la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9277ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam_grad(grad, beta1, beta2, n_iter): #n_iter is the number of the iteration/ epoch ?\n",
    "    first_moment = 0\n",
    "    second_moment = 0\n",
    "    first_moment = beta1 * first_moment+(1-beta1)*grad\n",
    "    second_moment = beta2* second_moment + (1-beta2)*grad*grad\n",
    "    first_unbias = first_moment / (1-beta1**n_iter)\n",
    "    second_unbias = second_moment / (1-beta2**n_iter)\n",
    "    return (first_unbias, second_unbias)\n",
    "\n",
    "def learning_rate_decay(learning_rate_0, n_epoch, N_epoch):\n",
    "    learning_rate_n = learning_rate_0*(1+np.cos(np.pi*n_epoch/N_epoch))/2  # Commence à 1\n",
    "    return learning_rate_n\n",
    "\n",
    "\n",
    "def update_parameters(params, grads, learning_rate, n_epoch, N_epoch, beta1, beta2, learning_r_decay=True, adam=True):\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    if learning_r_decay:\n",
    "        learning_rate_cur = learning_rate_decay(learning_rate, n_epoch, N_epoch)\n",
    "    else:\n",
    "        learning_rate_cur = learning_rate\n",
    "    for l in range(1,L+1):\n",
    "        if adam:\n",
    "            first_unbias_w, second_unbias_w = adam_grad(grads['dW' + str(l)], beta1, beta2, n_epoch)\n",
    "            parameters['W' + str(l)] = parameters['W' + str(l)] -learning_rate*first_unbias_w / (np.sqrt(second_unbias_w)+ 1e-7)\n",
    "            first_unbias_l, second_unbias_l = adam_grad(grads['db' + str(l)], beta1, beta2, n_epoch)\n",
    "            parameters['b' + str(l)] = parameters['b' + str(l)] -learning_rate*first_unbias_l / (np.sqrt(second_unbias_l)+ 1e-7)\n",
    "        else:\n",
    "            parameters['W' + str(l)] = parameters['W' + str(l)] -learning_rate_cur*grads['dW' + str(l)]\n",
    "            parameters['b' + str(l)] = parameters['b' + str(l)] -learning_rate_cur*grads['db' + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9ec0a",
   "metadata": {},
   "source": [
    "## Modèle complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abddcf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_layer_model\n",
    "def L_layer_model(X, Y, parameters, learning_rate, lambda_reg, n_epoch, N_epoch, beta1, beta2, prev_cost,\n",
    "                  learning_r_dec, adam):\n",
    "\n",
    "    costn_1 = prev_cost\n",
    "    AL, caches = L_model_forward(X, parameters)\n",
    "    cost = compute_cost(AL, Y, 'softmax', parameters, lambda_reg)\n",
    "    grads = L_model_backward(AL, Y, caches)\n",
    "    parameters = update_parameters(parameters, grads, learning_rate, n_epoch, N_epoch, beta1, beta2, learning_r_dec, adam)\n",
    "\n",
    "    if costn_1 < cost:\n",
    "        print(\"Error, Alpha parameter to lessen\")\n",
    "\n",
    "    return parameters, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0570539",
   "metadata": {},
   "source": [
    "## Fonction d'évaluation de la performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6775674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(Ypred, Ytest):\n",
    "    tot_true = np.sum(Ytest==np.array(np.argmax(Ypred, axis=0)))\n",
    "    return tot_true/Ytest.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd256a8",
   "metadata": {},
   "source": [
    "## Tracé de l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fb4643b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_function(cost):\n",
    "    n_iter = len(cost)\n",
    "    iter = np.arange(1, n_iter+1)\n",
    "    plt.plot(iter, cost)\n",
    "    plt.title(\"Cost function\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracy(train_accu, test_accu):\n",
    "    n_iter = len(train_accu)\n",
    "    iter = np.arange(1, n_iter+1)\n",
    "    plt.plot(iter, train_accu, \"b--\", label=\"Training accuracy\")\n",
    "    plt.plot(iter, test_accu, \"r--\", label=\"Test accuracy\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Accuracy\")\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cbcad9e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9424437430367316\n",
      "2.9205603391031563\n",
      "3.14267259243959\n",
      "2.9354063609786354\n",
      "2.9930795385596256\n",
      "2.9365973454518217\n",
      "3.055950356216418\n",
      "2.8609757655962285\n",
      "2.9539781139782537\n",
      "3.1622771579900437\n",
      "2.857226299512101\n",
      "3.0332455874367015\n",
      "3.0267255477212025\n",
      "2.8840601402217363\n",
      "2.8437326395451943\n",
      "2.7500524690348787\n",
      "2.718004109607297\n",
      "3.057639432754966\n",
      "2.910746533433625\n",
      "3.1169310775613965\n",
      "3.0451152899176384\n",
      "2.8662814894440185\n",
      "3.039308719023365\n",
      "2.875299786381128\n",
      "3.059748910684859\n",
      "2.9226358915831567\n",
      "2.92718862622531\n",
      "3.0169071548697\n",
      "2.932011280886143\n",
      "2.8440414686723976\n",
      "3.1445948739221\n",
      "2.960871095431054\n",
      "2.8483752153189013\n",
      "3.0898720732371263\n",
      "3.0534091956677\n",
      "2.74378166134304\n",
      "2.849680481575449\n",
      "2.8970465358605346\n",
      "3.1473388084395326\n",
      "3.0071607983277904\n",
      "3.1535713455663497\n",
      "2.9738768165325187\n",
      "2.717683584251465\n",
      "2.707854155953211\n",
      "2.9831303819463044\n",
      "2.834741224542766\n",
      "3.1288981726085314\n",
      "3.001398803227531\n",
      "3.0055157674562505\n",
      "3.007069059515583\n",
      "2.9483510008758045\n",
      "3.1043875825815697\n",
      "3.0518194768862275\n",
      "2.734702374712584\n",
      "3.011882441359477\n",
      "2.910848161656545\n",
      "3.002517042858228\n",
      "3.0243775670882163\n",
      "3.031783003052138\n",
      "2.6948023908102607\n",
      "3.0046920684501246\n",
      "2.860270068784744\n",
      "2.8856384598173497\n",
      "3.0011515557057056\n",
      "2.821635872532458\n",
      "2.9610908890322833\n",
      "3.100193745698241\n",
      "3.0608582640647004\n",
      "2.810019198776964\n",
      "2.860970497485458\n",
      "2.917530770836872\n",
      "2.979058513213921\n",
      "2.9278624837012086\n",
      "3.122807276181634\n",
      "2.9226045637829845\n",
      "3.072545589189499\n",
      "2.98214159845406\n",
      "2.7090464030003547\n",
      "3.0143347530824527\n",
      "2.8690903472344944\n",
      "3.001502583747339\n",
      "2.898976023076597\n",
      "2.7578590123608846\n",
      "2.901243032575241\n",
      "2.959734268266413\n",
      "3.126797739680873\n",
      "3.0135943575610176\n",
      "2.8222338484926066\n",
      "3.1350944750602556\n",
      "2.912652289834166\n",
      "3.0213914343985557\n",
      "2.728125730922383\n",
      "3.0009593683326967\n",
      "2.7512913985075222\n",
      "2.912184010515032\n",
      "2.868865788586412\n",
      "2.937298478055227\n",
      "2.743830646786062\n",
      "2.8318695687056548\n",
      "2.8595352711976387\n",
      "3.0337112560906876\n",
      "2.8400541601218916\n",
      "2.8230780121328296\n",
      "3.0248798351288144\n",
      "2.782347244666341\n",
      "2.959529986041464\n",
      "2.9879488031834307\n",
      "2.8516235074920817\n",
      "2.862278114936669\n",
      "2.8983794108340355\n",
      "3.0342292630054803\n",
      "2.9924311940724047\n",
      "2.9878848586558657\n",
      "2.8808202113715384\n",
      "2.813259997805838\n",
      "2.8360031219602435\n",
      "2.777315103463077\n",
      "2.9879371717899077\n",
      "2.951716348244584\n",
      "2.8920210765799634\n",
      "2.84201714271924\n",
      "2.7575486392711843\n",
      "2.8940705308762693\n",
      "2.825589904159121\n",
      "2.9393711922636583\n",
      "2.991980088456935\n",
      "2.684038269310463\n",
      "2.820967427158508\n",
      "2.9640059381375314\n",
      "2.9051750258533335\n",
      "2.9590563963158676\n",
      "3.0640907502489982\n",
      "3.006345045963233\n",
      "2.9415534711060563\n",
      "2.975481539634852\n",
      "2.844316684127815\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25616/4074326149.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mplot_accuracy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_accu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_accu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m NN_model(X_train, Y_train, X_test_clean, Y_test_clean, N_epoch= 100, m_batch = 128, learning_rate=0.0001, lambda_reg=0,\n\u001b[0m\u001b[0;32m     50\u001b[0m          beta1=0.9, beta2 = 0.999, learning_r_dec= False, adam=False)\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25616/4074326149.py\u001b[0m in \u001b[0;36mNN_model\u001b[1;34m(X_train, Y_train, X_test_clean, Y_test_clean, N_epoch, m_batch, learning_rate, lambda_reg, beta1, beta2, learning_r_dec, adam)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m             \u001b[0mpred_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL_model_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL_X_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_final\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m             \u001b[0mpred_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL_model_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_clean\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test_clean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m             \u001b[0mtrain_accu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mtest_accu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25616/2036476840.py\u001b[0m in \u001b[0;36mL_model_forward\u001b[1;34m(X, parameters)\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mbl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mL\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m# AL, cache = linear_activation_forward(A_prev, Wl, bl, activation=\"sigmoid\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25616/2036476840.py\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[1;34m(A_prev, W, b, activation)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"relu\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlinear_cache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation_cache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25616/2036476840.py\u001b[0m in \u001b[0;36mlinear_forward\u001b[1;34m(A, W, b)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mmean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mA\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "np.random.seed(1)  # pour que l'exécution soit déterministe\n",
    "\n",
    "data, labels = read_cifar(short=False)\n",
    "data = (data - np.mean(data, axis=0))/np.std(data, axis=0)\n",
    "Xapp, Yapp, Xtest, Ytest = split_data(data, labels)\n",
    "\n",
    "X_train = Xapp.T\n",
    "Y_train = np.array([Yapp])\n",
    "X_test_clean = Xtest.T\n",
    "Y_test_clean = np.array([Ytest])\n",
    "\n",
    "def NN_model(X_train, Y_train, X_test_clean, Y_test_clean, N_epoch, m_batch, learning_rate, lambda_reg, \n",
    "              beta1=0.9, beta2 = 0.999, learning_r_dec= True, adam=True):\n",
    "    n_x, m = X_train.shape[0], X_train.shape[1]\n",
    "    assert m % m_batch ==0\n",
    "    n_batch = m // m_batch\n",
    "    L_X_train = np.split(X_train, n_batch, axis=1)\n",
    "    L_Y_train = np.split(Y_train, n_batch, axis=1)\n",
    "    uniques = np.unique(Yapp)\n",
    "    n_y = len(uniques)\n",
    "    costs = []\n",
    "    train_accu = []\n",
    "    test_accu = []\n",
    "    layers_dims = [n_x, 50, 30, n_y] # 0.0025 good for [50,25]  # Try 0.0001 bcs 0.0005 not working at 1500 it.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    for k in range(1,N_epoch+1):\n",
    "        for j in range(n_batch):\n",
    "            Y_train_final = np.zeros((n_y, L_Y_train[j].shape[1]))\n",
    "            cost = np.inf\n",
    "            for i in range(n_y):\n",
    "                Y_train_final[i,:] = (L_Y_train[j] == uniques[i])\n",
    "                \n",
    "            parameters, cost = L_layer_model(L_X_train[j], Y_train_final, parameters, learning_rate, lambda_reg, \n",
    "                                   k, N_epoch, beta1, beta2, cost, learning_r_dec, adam)\n",
    "            costs.append(cost)\n",
    "            \n",
    "            pred_train = evaluate_prediction(L_model_forward(L_X_train[j],parameters)[0], Y_train_final)\n",
    "            pred_test = evaluate_prediction(L_model_forward(X_test_clean,parameters)[0], Y_test_clean)\n",
    "            train_accu.append(pred_train)\n",
    "            test_accu.append(pred_test)\n",
    "            print(cost)\n",
    "        print(f'NN accuracy on training set \\033[1m{pred_train:.3%}\\033[0m')\n",
    "        print(f'NN accuracy on test set \\033[1m{pred_test:.3%}\\033[0m')\n",
    "        plot_loss_function(cost)\n",
    "        plot_accuracy(train_accu, test_accu)\n",
    "\n",
    "NN_model(X_train, Y_train, X_test_clean, Y_test_clean, N_epoch= 100, m_batch = 128, learning_rate=0.0001, lambda_reg=0,\n",
    "         beta1=0.9, beta2 = 0.999, learning_r_dec= False, adam=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c6d922",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = read_cifar(short=True)\n",
    "data = (data - np.mean(data, axis=0))/np.std(data, axis=0)\n",
    "Xapp, Yapp, Xtest, Ytest = split_data(data, labels)\n",
    "\n",
    "X_train = Xapp.T\n",
    "Y_train = np.array([Yapp])\n",
    "X_test_clean = Xtest.T\n",
    "Y_test_clean = np.array([Ytest])\n",
    "L_X_train = np.array_split(X_train, n_batch, axis=1)\n",
    "len(L_X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e9700cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
