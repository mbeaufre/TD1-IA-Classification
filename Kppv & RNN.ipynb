{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a071d8d5",
   "metadata": {},
   "source": [
    "# TD1 – Kppv et réseaux de neurones pour la classification d'images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52e6a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from kppv import *\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc9a48",
   "metadata": {},
   "source": [
    "## Initialisation\n",
    "On commence par définir une fonction, qui, à partir d'une liste contenant les dimensions des différentes couches du réseau de neurones, initialisera les paramètres.\n",
    "Pour avoir une initialisation reproductible on fixe np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cf5ad19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)  # number of layers in the network\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "\n",
    "        assert (parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert (parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1ae34",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "Ensuite on définit 3 fonctions nécessaires à la propagation en avant. Une fonction **linear_forward** effectuant les produits matriciels de la propagation en avant, puis une fonction **linear_activation_forward**, qui, utilisant **linear_forward** effectue la propagation d'une couche à une autre (produit matriciel + fonction d'activation). On a 3 options de fonctions : relu, sigmoid ou softmax. Par la suite nous utiliserons la fonctions **ReLU** pour les couches cachées, et la fonction **softmax** pour générer les probabilités sur la couche de sortie.\n",
    "Enfin, **L_model_forward** permet de réaliser la propagation à travers toutes les couches, en utilisant **linear_activation_forward**. C'est le seul endroit où un boucle *for* est inévitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5908b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    if activation == \"sigmoid\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"softmax\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    elif activation == \"relu\":\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the neural network\n",
    "    for l in range(1, L + 1): # L+1 is pretty important ATTENTION\n",
    "        A_prev = A\n",
    "        Wl = parameters['W' + str(l)]\n",
    "        bl = parameters['b' + str(l)]\n",
    "        if l < L:\n",
    "            A, cache = linear_activation_forward(A_prev, Wl, bl, activation=\"relu\")\n",
    "        else:\n",
    "            # AL, cache = linear_activation_forward(A_prev, Wl, bl, activation=\"sigmoid\")\n",
    "            AL, cache = linear_activation_forward(A_prev, Wl, bl, activation=\"softmax\")\n",
    "        caches.append(cache)\n",
    "    return AL, caches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0559fc5",
   "metadata": {},
   "source": [
    "## Fonction de coût\n",
    "On définit une fonction de coût. On a J = $\\frac{-1}{m}$ $\\sum_{i=1}^{L} y_i*log(al_i) $ où AL est le vecteur de sortie obtenue lors de la propagation en avant. C'est la fonction de coût dite de 'cross-entropy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23e8bc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y, activation_out):\n",
    "    m = Y.shape[1]\n",
    "    if activation_out == \"sigmoid\":\n",
    "        cost = (-1/m)*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))\n",
    "    elif activation_out == \"softmax\":\n",
    "        cost = (-1 / m) * np.sum(Y * np.log(AL))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efcd97f",
   "metadata": {},
   "source": [
    "## Backward propagation\n",
    "On définit 3 fonctions nécessaires à la backpropagation. Une fonction **linear_backward** effectuant les produits matriciels de la backpropagation, puis une fonction **linear_activation_backward**, qui, utilisant **linear_backward** propage la backpropagation en fonctione de la fonction d'activation de la couche (utilisation de la fonction backward adéquate).\n",
    "Les différentes fonctions backward sont définies dans le fichier function.py, afin de ne pas alourdir le propos ici. Elles prennent toutes dA en argument, ainsi que le cache, et retournent dZ.\n",
    "Enfin, **L_model_backward** permet de réaliser la backpropagation à travers toutes les couches, en utilisant **linear_activation_backward**. C'est le seul endroit où un boucle *for* est inévitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3512cbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    linear_cache, activation_cache = cache\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"softmax\":\n",
    "        dZ = softmax_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\n",
    "    grads['dA' + str(L-1)], grads['dW' + str(L)], grads['db' + str(L)] = linear_activation_backward(dAL, caches[L-1], activation=\"softmax\")  # Softmax or sigmoid, in function of the need\n",
    "    for l in reversed(range(L-1)):\n",
    "        grads['dA' + str(l)], grads['dW' + str(l+1)], grads['db' + str(l+1)] = linear_activation_backward(grads['dA' + str(l+1)], caches[l], activation=\"relu\")\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1677a039",
   "metadata": {},
   "source": [
    "## Mise à jour des paramètres\n",
    "Suite à la backpropagation, on définit une fonction pour mettre à jour les paramètres en fonctions des gradients récupérés, des valeurs initiales des paramètres, ainsi que du learning_rate. Ce learning_rate est un hyper-paramètre qu'il nous faudra ajuster par la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9277ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "    parameters = params.copy()\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "    for l in range(1,L+1):\n",
    "        parameters['W' + str(l)] = parameters['W' + str(l)] -learning_rate*grads['dW' + str(l)]\n",
    "        parameters['b' + str(l)] = parameters['b' + str(l)] -learning_rate*grads['db' + str(l)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b9ec0a",
   "metadata": {},
   "source": [
    "## Modèle complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abddcf12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L_layer_model\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False):\n",
    "    np.random.seed(1)\n",
    "    costs = []  # keep track of cost\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    costn_1 = np.inf\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        cost = compute_cost(AL, Y, activation_out='softmax')\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if costn_1 < cost:\n",
    "            print(\"Error, Alpha parameter to lessen\")\n",
    "            break\n",
    "        costn_1 = cost\n",
    "        # Print the cost every 100 iterations\n",
    "        if i < 10:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        elif print_cost and i % 100 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "            predictions = np.argmax(AL, axis=0)\n",
    "            Y_reversed_one_hot = np.argmax(Y, axis=0)\n",
    "            assert Y_reversed_one_hot.size == predictions.size\n",
    "            print(\"Accuracy after iteration {}: {}\".format(i, np.sum(Y_reversed_one_hot == predictions) / Y_reversed_one_hot.size))\n",
    "        if i % 100 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0570539",
   "metadata": {},
   "source": [
    "## Fonction d'évaluation de la performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6775674b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_prediction(Ytest, Ypred):\n",
    "    predictions = np.argmax(Ypred, axis=0)\n",
    "    assert Ytest.size == predictions.size\n",
    "    return np.sum(Ytest == Ypred) / Ytest.size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd256a8",
   "metadata": {},
   "source": [
    "## Tracé de l'apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a6228a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52b324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbcad9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='main':\n",
    "    # Training the model\n",
    "    np.random.seed(1)  # pour que l'exécution soit déterministe\n",
    "\n",
    "    data, labels = read_cifar(short=True)\n",
    "    Xapp, Yapp, Xtest, Ytest = split_data(data, labels)\n",
    "    X_train = Xapp.T\n",
    "    Y_train = np.array([Yapp])\n",
    "    X_test_clean = Xtest.T\n",
    "    Y_test_clean = np.array([Ytest])\n",
    "    n_x, m = X_train.shape[0], X_train.shape[1]\n",
    "    uniques = np.unique(Yapp)\n",
    "    n_y = len(uniques)\n",
    "    Y_train_final = np.zeros((n_y, Y_train.shape[1]))\n",
    "    for i in range(n_y):\n",
    "        Y_train_final[i,:] = (Yapp == uniques[i])\n",
    "    layers_dims = [n_x, 50, 30, n_y] # 0.0025 good for [50,25]  # Try 0.0001 bcs 0.0005 not working at 1500 it.\n",
    "    parameters, costs = L_layer_model(X_train, Y_train_final, layers_dims, learning_rate=0.0004, num_iterations=3000, print_cost=True)\n",
    "\n",
    "    pred_train = evaluate_prediction(X_train, Y_train, parameters)\n",
    "    pred_test = evaluate_prediction(X_test_clean, Y_test_clean, parameters)\n",
    "    print(f'NN accuracy on training set \\033[1m{evaluate_prediction(X_train, Y_train, parameters):.3%}\\033[0m')\n",
    "    print(f'NN accuracy on test set \\033[1m{evaluate_prediction(X_test_clean, Y_test_clean, parameters):.3%}\\033[0m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d96d3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06348349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
